{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_func(m):\n",
    "    svd = np.linalg.svd(m, compute_uv=False)\n",
    "\n",
    "    return np.sum(svd[2:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_min_knowns(n,prev_knowns=1):\n",
    "    \n",
    "    rank = 1\n",
    "\n",
    "    U = np.random.randn(n, rank)\n",
    "    V = np.random.randn(n, rank)\n",
    "    original = np.dot(U, V.T)\n",
    "\n",
    "    for knowns in range(np.max([prev_knowns - 20, 1]), n*n):\n",
    "        max_steps = 100000\n",
    "        threshold = 1 # distance between solution and answer when they are considered the same\n",
    "\n",
    "        unknowns = n*n - knowns\n",
    "\n",
    "        #print('>',n*n, unknowns)\n",
    "        \n",
    "        mask = np.array([0] * unknowns + [1] * (n*n - unknowns))\n",
    "        np.random.shuffle(mask)\n",
    "        mask = np.reshape(mask, [n,n])\n",
    "        mask = np.ma.make_mask(mask)\n",
    "\n",
    "\n",
    "        \n",
    "        # First try Newton's method:\n",
    "\n",
    "        def comp_grad(m, boolMask, obj_func):\n",
    "            \"\"\" Computes gradient that maximizes the objective function \"\"\"\n",
    "            epsilon = 1e-3\n",
    "\n",
    "            # Yes, grad is a vector now\n",
    "            grad = []\n",
    "\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    if boolMask[i,j] and np.random.random() > 1 - portion:\n",
    "                        diff = np.zeros([n,n])\n",
    "                        diff[i,j] = epsilon\n",
    "                        grad.append((obj_func(m + diff) - obj_func(m - diff))/(2*epsilon))\n",
    "\n",
    "            return grad\n",
    "\n",
    "        def comp_hessian(m, boolMask, of):\n",
    "            \"\"\" Computes hessian (only diagonal) \"\"\"\n",
    "            epsilon = 1e-3\n",
    "\n",
    "            hessian = []\n",
    "\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    if boolMask[i,j]:\n",
    "                        row = []\n",
    "\n",
    "                        diff = np.zeros([n,n])\n",
    "                        diff[i,j] = epsilon\n",
    "                        hessian.append((of(m + diff) + of(m - diff) - 2*of(m))/epsilon**2)\n",
    "\n",
    "            return hessian\n",
    "\n",
    "\n",
    "        starting_point = np.copy(original)\n",
    "\n",
    "        boolMask = np.ma.make_mask(np.where(np.array(mask) < 0.5, 1, 0))\n",
    "        starting_point[boolMask] = 0\n",
    "\n",
    "        prev_norm = np.linalg.norm(starting_point,'nuc')\n",
    "\n",
    "        norms = []\n",
    "        distances = []\n",
    "        sing_vals = []\n",
    "\n",
    "        current_point = starting_point\n",
    "\n",
    "        #pl.imshow(np.abs((current_point-original)/original), cmap=plt.get_cmap('hot'),\n",
    "        #          interpolation='nearest', vmin=0, vmax=1)\n",
    "        #pl.colorbar()\n",
    "\n",
    "\n",
    "        initial_s = 4\n",
    "        s = 4\n",
    "        portion = 1.1\n",
    "        for i in range(max_steps):\n",
    "            cur_norm = np.linalg.norm(current_point,'nuc')\n",
    "            norms.append(cur_norm)\n",
    "            sing_vals.append(np.linalg.svd(current_point, compute_uv=False))\n",
    "            distances.append(np.linalg.norm(current_point-original,'fro'))\n",
    "            diff = cur_norm - prev_norm\n",
    "\n",
    "            # portion really should depend on s since smaller s implies the need of a more accurate gradient estimate\n",
    "            #portion = 1 - np.exp(-1/(20*s))\n",
    "            #if i % 1000 == 0:\n",
    "            #print(i, cur_norm, diff, np.linalg.norm(current_point-original,'fro'), initial_s, s, portion)\n",
    "            prev_norm = cur_norm\n",
    "            \n",
    "            if np.linalg.norm(current_point-original,'fro') < threshold:\n",
    "                return knowns\n",
    "            \n",
    "            if diff >= 0 and i > 1:\n",
    "                break\n",
    "\n",
    "            ### slowest part of the code ###\n",
    "            descent = np.linalg.lstsq(np.diag(comp_hessian(current_point,boolMask,obj_func)),\n",
    "               comp_grad(current_point,boolMask,obj_func), rcond=None)[0]\n",
    "\n",
    "            descent_matrix = np.zeros([n,n])\n",
    "            count = 0\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    if boolMask[i,j]:\n",
    "                        descent_matrix[i,j] = descent[count]\n",
    "                        count = count + 1\n",
    "\n",
    "            reg = 0.39\n",
    "            next_point = current_point - reg*descent_matrix\n",
    "\n",
    "            current_point = next_point\n",
    "\n",
    "        # Next try gradient descent:\n",
    "        \n",
    "        def comp_grad(m,boolMask):\n",
    "            \"\"\" Computes gradient that maximizes the objective function \"\"\"\n",
    "            epsilon = 1e-3\n",
    "\n",
    "            grad = np.zeros([n,n])\n",
    "\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    if boolMask[i,j]:\n",
    "                        diff = np.zeros([n,n])\n",
    "                        diff[i,j] = epsilon\n",
    "                        grad[i,j] = (obj_func(m + diff) - obj_func(m - diff))/(2*epsilon)\n",
    "\n",
    "            return grad\n",
    "        \n",
    "        def line_search(point, grad, obj_func, s = 4, threshold = 1e-10):\n",
    "            \"\"\" Finds a maximum step size that maximizes the descent (since computing gradient is hard) \"\"\"\n",
    "\n",
    "            choice = point - grad * s\n",
    "            while obj_func(choice) > obj_func(point):\n",
    "                s = s / 2\n",
    "                choice = point - grad * s\n",
    "\n",
    "            initial_s = s\n",
    "\n",
    "            while True:\n",
    "                left = choice + grad * s/2\n",
    "                right = point - grad * s/2\n",
    "\n",
    "                s = s / 2\n",
    "                if obj_func(left) < obj_func(right):\n",
    "                    choice = left\n",
    "                else:\n",
    "                    choice = right\n",
    "\n",
    "                if np.abs(obj_func(left) - obj_func(right)) < threshold:\n",
    "                    break\n",
    "\n",
    "            return (choice, initial_s, s)\n",
    "\n",
    "        starting_point = np.copy(original)\n",
    "\n",
    "        boolMask = np.ma.make_mask(np.where(np.array(mask) < 0.5, 1, 0))\n",
    "        starting_point[boolMask] = 0\n",
    "\n",
    "        prev_norm = np.linalg.norm(starting_point,'nuc')\n",
    "\n",
    "        norms = []\n",
    "        distances = []\n",
    "        sing_vals = []\n",
    "\n",
    "        current_point = starting_point\n",
    "\n",
    "        initial_s = 4\n",
    "        s = 4\n",
    "        portion = 1.1\n",
    "        for i in range(threshold):\n",
    "            cur_norm = np.linalg.norm(current_point,'nuc')\n",
    "            norms.append(cur_norm)\n",
    "            sing_vals.append(np.linalg.svd(current_point, compute_uv=False))\n",
    "            distances.append(np.linalg.norm(current_point-original,'fro'))\n",
    "            diff = cur_norm - prev_norm\n",
    "\n",
    "            #print(i, cur_norm, diff, np.linalg.norm(current_point-original,'fro'))\n",
    "            prev_norm = cur_norm\n",
    "            \n",
    "            if np.linalg.norm(current_point-original,'fro') < threshold:\n",
    "                return knowns\n",
    "            \n",
    "            if diff >= 0 and i > 1:\n",
    "                break\n",
    "\n",
    "            ### slowest part of the code ###\n",
    "            grad = comp_grad(current_point,boolMask)\n",
    "            current_point, initial_s, s = line_search(current_point, grad, obj_func, s = initial_s)\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_min_knowns(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 15\n",
      "5 23\n",
      "6 33\n",
      "7 37\n",
      "8 45\n",
      "9 76\n",
      "10 96\n",
      "11 117\n",
      "12 138\n",
      "13 165\n",
      "14 187\n",
      "15 217\n",
      "16 246\n",
      "17 283\n",
      "18 318\n",
      "19 355\n",
      "20 395\n",
      "21 440\n",
      "22 482\n",
      "23 527\n",
      "24 572\n",
      "25 621\n",
      "26 671\n",
      "27 725\n",
      "28 779\n",
      "29 837\n",
      "30 892\n",
      "31 960\n"
     ]
    }
   ],
   "source": [
    "n_vals = []\n",
    "min_knowns_vals = []\n",
    "\n",
    "prev = 1\n",
    "for n in range(4,10000):\n",
    "    min_knowns = get_min_knowns(n, prev)\n",
    "    if min_knowns:\n",
    "        prev = min_knowns\n",
    "        n_vals.append(n)\n",
    "        min_knowns_vals.append(min_knowns)\n",
    "        print(n, min_knowns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2ed91eb8d0>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(n_vals, min_knowns_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 15\n",
      "5 23\n",
      "6 35\n",
      "7 45\n",
      "8 59\n",
      "9 78\n",
      "10 99\n",
      "11 118\n",
      "13 167\n",
      "15 157\n",
      "16 192\n",
      "17 188\n",
      "18 229\n",
      "19 222\n",
      "20 268\n",
      "21 253\n",
      "22 296\n",
      "23 303\n",
      "24 326\n",
      "25 334\n",
      "26 343\n",
      "27 359\n",
      "28 414\n",
      "29 399\n",
      "30 455\n",
      "31 466\n",
      "32 537\n"
     ]
    }
   ],
   "source": [
    "n_vals = []\n",
    "min_knowns_vals = []\n",
    "\n",
    "prev = 1\n",
    "for n in range(4,10000):\n",
    "    min_knowns = get_min_knowns(n, prev)\n",
    "    if min_knowns:\n",
    "        prev = min_knowns\n",
    "        n_vals.append(n)\n",
    "        min_knowns_vals.append(min_knowns)\n",
    "        print(n, min_knowns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
